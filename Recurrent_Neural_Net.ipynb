{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "In this jupyter notebook we will focus on $\\textit{Recurrent Neural Network}$ which is a neural network that makes use of sequential information. It uses the order of inputs to create a logical connection between them, which is very useful in tasks like natural language processing, speech recognition and video activity recognition. \n",
    "\n",
    "\n",
    "<img src=\"rec_neu_net/rec_net.png\"  style=\"width=:70% ;height=:70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "For a given set $D = \\{(x_n, y_n)\\}_{n=1}^N$ classify each $\\textit{y}_t$ for the new $\\textbf{x}$. Where $\\textit{y}_t$ is a single 'word' of whole output sequence $\\textit{y}$.\n",
    "\n",
    "<h3>Examples</h3>\n",
    "\n",
    "\n",
    "<img src=\"rec_neu_net/thanks_to_Andrew.png\" style=\"width=:70% ;height=:70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Example:\n",
    "\n",
    "We are going to create Recurrent Neural Network (RNN) model to add binary numbers up to 256.\n",
    "<img src=\"rec_neu_net/binary_summation.gif\" style=\"width:30% ;height:30%;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data\n",
    "First of all we need to prepare some data for training. To do so we will create a dictionary that maps integers for binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_int2bin_dict(binary_dim=8):\n",
    "    '''\n",
    "    returns a dictionary of integers tranlsated to binaries\n",
    "    '''\n",
    "    int2binary = {}\n",
    "\n",
    "    largest_number = pow(2, binary_dim)\n",
    "    binary = np.unpackbits(\n",
    "        np.array([range(largest_number)], dtype=np.uint8).T, axis=1)\n",
    "    for i in range(largest_number):\n",
    "        int2binary[i] = binary[i]\n",
    "    return int2binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer: 5, binary format: [0 0 0 0 0 1 0 1]\n",
      "Integer: 109, binary format: [0 1 1 0 1 1 0 1]\n",
      "Integer: 21, binary format: [0 0 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "int2binary = create_int2bin_dict()\n",
    "\n",
    "integers = [5, 109, 21]\n",
    "for integer in integers:\n",
    "    print('Integer: {}, binary format: {}'.format(integer, int2binary[integer]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model\n",
    "\n",
    "We will create RNN with one hidden layer. RNN differs from simple feedforward NN as the output is dependent not only from input, but also from previous computations. We need to provide data from previous iterations on single sequence.\n",
    "\n",
    "<img src=\"rec_neu_net/rec_net_explained.png\" style=\"width:60% ;height:60%;\">\n",
    "\n",
    "Notice that on the image Wxa are weights between input and hidden layer, Wah are weights between hidden layer and Waa are weights between hidden layers.\n",
    "\n",
    "Just like in feedforward neural network we are in need of some non-linear functions like sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid\n",
    "We are going to use sigmoid non-linear function, that is defined:\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "And looks like this below:\n",
    "<img src=\"rec_neu_net/sigmoid.jpg\" height=\"40%\" width=\"40%\">\n",
    "\n",
    "We also need to know sigmoid derivative which is equal to:\n",
    "$\\frac{\\delta \\sigma}{\\delta x} = \\sigma(x) \\cdot (1 - \\sigma(x)) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    param x: input matrix of size NxM\n",
    "    '''\n",
    "    return 1/(1+np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid for small value f.e. -100 equals: 0.000\n",
      "Sigmoid of big values f.e. 100 equals: 1.000\n"
     ]
    }
   ],
   "source": [
    "print('Sigmoid for small value f.e. -100 equals: {:.3f}'.format(sigmoid(-100)))\n",
    "print('Sigmoid of big values f.e. 100 equals: {:.3f}'.format(sigmoid(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation:\n",
    "$x^{<t>}$ - the $t^{th}$ input of the sequence\n",
    "\n",
    "$z^{<t>}$ - values in activation layer before non-linear function at $t^{th}$ element of the sequence\n",
    "\n",
    "$a^{<t>}$ - values in activation layer at $t^{th}$ element of the sequence, that equals $\\sigma(z^{<t>})$\n",
    "\n",
    "$y^{<t>}$ - true output for $x^{<t>}$ input \n",
    "\n",
    "$h^{<t>}$ - output layer vaule for $x^{<t>}$ input before non-linear function\n",
    "\n",
    "$\\overline{y}^{<t>}$ - predicted output for $x^{<t>}$ input after activation with non-linear function, that equals $\\sigma(h^{<t>})$\n",
    "\n",
    "\n",
    "$a^{<t-1>}$ - values in activation layer at $(t-1)^{th}$ element of the sequence (*for t=0 element in the sequence we assume $a^{<t-1>}$ equals zero)\n",
    "\n",
    "$W_{aa}$ - weights between $a^{<t-1>}$ and $a^{<t>}$ in the sequence.\n",
    "\n",
    "$W_{xa}$ - weights between input layer and hidden layer\n",
    "\n",
    "$W_{ay}$ - weights between hidden layer and output layer\n",
    "\n",
    "$\\sigma(z)$ - sigmoid non-linear function on variable z\n",
    "\n",
    "$T $ - whole lenght of a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables initialization\n",
    "\n",
    "We need to initialize weights and biases for our network, as matrices filled with random values of proper sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_random_weights(input_dim, hidden_dim, output_dim):\n",
    "    '''\n",
    "    param input_dim: number of cells in input layer\n",
    "    param output_dim: number of cells in output layer \n",
    "    param hidden_dim: number of nods in hidden layer\n",
    "    return: tuple of Wxa, Waa, Way\n",
    "    '''\n",
    "    np.random.seed(100)\n",
    "    Wxa = 2 * np.random.random((input_dim, hidden_dim)) - 1\n",
    "    Waa = 2*np.random.random((hidden_dim, hidden_dim))-1\n",
    "    Way = 2 * np.random.random((hidden_dim, output_dim)) - 1\n",
    "\n",
    "\n",
    "    return (Wxa, Waa, Way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wxa (from input layer to hidden layer) shape: (2, 10)\n",
      "Waa (from previous execution of hidden layer to current hidden layer) shape: (10, 10)\n",
      "Way (from hidden layer to output layer) shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "#example usage\n",
    "Wxa, Waa, Way = initialize_random_weights(input_dim = 2, hidden_dim = 10, output_dim=1)\n",
    "\n",
    "print(\"Wxa (from input layer to hidden layer) shape: {}\".format(Wxa.shape))\n",
    "print(\"Waa (from previous execution of hidden layer to current hidden layer) shape: {}\".format(Waa.shape))\n",
    "print(\"Way (from hidden layer to output layer) shape: {}\".format(Way.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward for single element - steps and description\n",
    "\n",
    "<h3>Steps: </h3>\n",
    "1. $z^{<t>} = W_{aa} \\times a^{<t-1>} + W_{xa} \\times x^{<th>}$\n",
    "2. $a^{<t>} = \\sigma(z^{<t>})$\n",
    "3. $\\overline{y}^{<t>} = \\sigma(W_{ay} \\times a^{<t>})$\n",
    "\n",
    "We need to compute values at hidden layer for $t^{th}$ element using $x^{<t>}$ input and $a^{<t-1>}$ hidden layer values, and then with them we can compute $\\overline{y}^{<t>}$ output. \n",
    "\n",
    "<img src=\"rec_neu_net/feed_forward.gif\" height=\"60%\" width=\"60%\">\n",
    "\n",
    "As we can see on this animation values at hidden layer depends not only on input, but also on values of hidden layers for previous inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(a, b, W):\n",
    "    '''\n",
    "    param a: input vector - binary number\n",
    "    param b: input vector - 2nd binary number \n",
    "    param W: tuple of Wxa, Waa, Way\n",
    "    return: pair of lists y_values, hidden_values \n",
    "    '''\n",
    "    binary_dim = a.shape[0]\n",
    "    \n",
    "    Wxa, Waa, Way = W\n",
    "    \n",
    "    hidden_dim = Waa.shape[0]\n",
    "    \n",
    "    hidden_values = []\n",
    "    hidden_values.append(np.zeros((1, hidden_dim)))\n",
    "    \n",
    "    y_values = [] \n",
    "    \n",
    "    for t in range(binary_dim):\n",
    "        x_t = np.array([[a[binary_dim - t - 1], b[binary_dim - t - 1]]]) \n",
    "\n",
    "        z_t = x_t @ Wxa + hidden_values[-1] @ Waa\n",
    "        a_t = sigmoid(z_t)\n",
    "\n",
    "        h_t = a_t @ Way\n",
    "        y_t = sigmoid(h_t)\n",
    "\n",
    "        y_values.append(y_t)\n",
    "        hidden_values.append(a_t)\n",
    "        \n",
    "    \n",
    "    return y_values, hidden_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function:\n",
    "\n",
    "At start weights are random numbers, so we need to find parameters that will predict output with the smallest error. we are going to search for them using gradient descent, which is a method that allows to reach local minimum by substracting derivatives values from weights.\n",
    "\n",
    "Before computing the gradient we need to define cost function, which in our case will be cross-entropy, but we could use mean squared error as well for this task:  $J = \\frac{1}{N}\\sum_{i=0}^N \\big( y_i \\cdot \\ln{\\overline{y}_i + (1-y_i)\\cdot(\\ln{(1-\\overline{y}_i)}) } \\big)$\n",
    "\n",
    "The cost function will let us measure how bad our model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "After defining cost function we can go to the part that is responsible for learning parameters - called backpropagation, with use of gradient descent.\n",
    "\n",
    "<h3>Gradient descent steps in sequence</h3>\n",
    "\n",
    "1. Compute the error on $\\overline{y}$ as derivative of the cost function (gradient)\n",
    "2. Compute the error on output layer before activation\n",
    "3. Compute the gradient of weights $W_{ay}$\n",
    "4. Compute the error on $a$ using chain rule from output and error from next execution of hidden layer. For last layer we assume that error on 'next' execution equals 0 as it doesn't exist.\n",
    "5. Compute the error on $z$ from $a$\n",
    "6. Compute the gradient of weights $W_{aa}$\n",
    "7. Compute the gradient of weights $W_{xa}$ \n",
    "\n",
    "<h3>Gradient descent in steps for people familiar with calculus</h3>\n",
    "\n",
    "1) Error on $\\overline{y}^{<t>} = \\frac{\\delta J}{\\delta \\overline{y}^{<t>}} = \\frac{\\delta y^{<t>}}{\\delta\\overline{y}^{<t>}} - \\frac{1-y^{<t>}}{(1-\\overline{y}^{<t>})} = \n",
    "\\frac{y^{<t>}-\\overline{y}^{<t>}}{\\overline{y}^{<t>}\\cdot(\\overline{y}^{<t>}-1)}$\n",
    "\n",
    "\n",
    "2) Error on $h^{<t>} = \\frac{\\delta J}{\\delta h^{<t>}} =  \\frac{\\delta J}{\\delta \\overline{y}^{<t>}} \\cdot \\frac{\\delta \\overline{y}^{<t>}}{\\delta h^{<t>}} = \\frac{y^{<t>} - \\overline{y}^{<t>}}{\\overline{y}^{<t>}\\cdot(\\overline{y}^{<t>}-1)} \\cdot (\\overline{y}^{<t>}\\cdot(\\overline{y}^{<t>}-1)) = y^{<t>} - \\overline{y}^{<t>} $\n",
    "\n",
    "\n",
    "3) Gradient of $W_{ay} = \\frac{\\delta J}{\\delta W_{ay}} = \\sum_{t=0}^{T} \\big( \\frac{\\delta J}{\\delta \\overline{y}^{<t>}} \\cdot \\frac{\\delta \\overline{y}^{<t>}}{\\delta h^{<t>}} \\cdot \\frac{\\delta h^{<t>}}{\\delta W_{ay}} \\big) =\n",
    "\\sum_{t=0}^{T} \\big( \\frac{\\delta {J}}{\\delta h^{<t>}} \\cdot \\frac{\\delta h^{<t>}}{\\delta W_{ay}} \\big) =\n",
    "\\sum_{t=0}^{T} \\big( \\frac{\\delta {J}}{\\delta h^{<t>}} \\cdot a^{<t>} \\big)$, as $\\frac{\\delta h^{<t>}}{\\delta W_{ay}} = a^{<t>}$, because $h^{<t>} = a^{<t>} \\times W_{ay} $ \n",
    "\n",
    "\n",
    "4) Error on $a^{<t>} = \\frac{\\delta J}{\\delta a^{<t>}} = \\sum_{i=t}^T \\big( \\frac{\\delta J}{\\delta h^{<i>}} \\cdot \\frac{\\delta h^{<i>}}{\\delta a^{<t>}} \\big) = \\frac{\\delta J}{\\delta h^{<t>}} \\cdot \\frac{\\delta h^{<t>}}{\\delta a^{<t>}} + \\frac{\\delta J}{\\delta a^{<t+1>}} \\cdot \\frac{\\delta a^{<t+1>}}{\\delta a^{<t>}} \n",
    "= \\frac{\\delta J}{\\delta h^{<t>}} \\cdot W_{ay} + \\frac{\\delta J}{\\delta a^{<t+1>}} \\cdot W_{aa}\n",
    "$, as $\\frac{\\delta h^{<t>}}{\\delta a^{<t>}} = W_{ay}$ and $\\frac{\\delta a^{<t+1>}}{\\delta a^{<t>}}= W_{aa} $, because\n",
    "$h^{<t>} = a^{<t>} \\times W_{ay}$ and $ a^{<t+1>} = a^{<t>} \\times W_{aa}$\n",
    "\n",
    "<i>At this point you can get a little shock, but it only looks terrible. Calculating the first part of $\\frac{\\delta J}{\\delta a^{<t>}}$ that comes from $h^{<t>}$ layer works just like in feedforward neural network. Rest of the sum comes from passing the data from hidden layer$^{<t>}$ to hidden layer in execution$^{<t+1>}$. Anyway - all you need to do is to calculate the error on last execution of hidden layer, that equals 0 + the part from $h^{<T>}$ layer. And then use this error to calculate error on execution$^{<T-1>}$ and go again and again backward until you get error on first execution  of hidden layer ($a^{<0>}$)</i>\n",
    "\n",
    "\n",
    "5) Error on $z^{<t>} = \\frac{\\delta J}{\\delta z^{<t>}} = \\frac{\\delta J}{\\delta a^{<t>}} \\cdot \\frac{\\delta a^{<t>}}{\\delta z^{<t>}} = \\frac{\\delta J}{\\delta a^{<t>}} \\cdot a^{<t>}(1-a^{<t>})$, as $\\frac{\\delta a^{<t>}}{\\delta z^{<t>}} = a^{<t>}\\cdot(1-a^{<t>}) $, because $a^{<t>} = \\sigma(z^{<t>})$\n",
    "\n",
    "\n",
    "6) Gradient of $W_{aa} = \\frac{\\delta J}{\\delta W_{aa}} = \\sum_{t=0}^T \\big(\\frac{\\delta J}{\\delta z^{<t>}} \\cdot \\frac{\\delta z^{<t>}}{\\delta W_{aa}} \\big)  = \\sum_{t=0}^T \\big(\\frac{\\delta J}{\\delta z^{<t>}}\\cdot a^{<t-1>} \\big)$, as $\\frac{\\delta z^{<t>}}{\\delta W_{aa}} = a^{<t-1>}$, because $z^{<t>} = x^{<t>} \\times W_{xa} + a^{<t-1>} \\times W_{aa} $\n",
    "\n",
    "\n",
    "7) Gradient of $W_{xa} = \\frac{\\delta J}{\\delta W_{xa}} = \\sum_{t=0}^T \\big(\\frac{\\delta J}{\\delta z^{<t>}} \\cdot \\frac{\\delta z^{<t>}}{\\delta W_{xa}}\\big) = \\sum_{t=0}^T\\big(\\frac{\\delta J}{\\delta z^{<t>}}\\cdot x^{<t>}\\big)$, as $\\frac{\\delta z^{<t>}}{\\delta W_{xa}} = x^{<t>}$, because $z^{<t>} = x^{<t>} \\times W_{xa} + a^{<t-1>} \\times W_{aa} $\n",
    "\n",
    "In the code you can see the same operations done on matrices. Somewhere there code will be a little bit diffrent due to matrices operations (to compute gradient sometimes it's needed to transpose matrix).\n",
    "\n",
    "<img src=\"rec_neu_net/backprop_through_time.gif\" height=\"60%\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(a, b, c, W, y_values, hidden_values):\n",
    "    '''\n",
    "    param a: ndarray- binary number, one of the inputs\n",
    "    param b: ndarray - binary number, second of the inputs\n",
    "    param c: ndarray - the true output\n",
    "    param W: tuple of weights Wax, Waa, Way\n",
    "    param y_values: ndarray of predicted output\n",
    "    param hidden_values: ndarray, activation layer values at each execution t\n",
    "    '''\n",
    "    binary_dim = a.shape[0]\n",
    "\n",
    "    error = 0\n",
    "\n",
    "    Wxa, Waa, Way = W\n",
    "\n",
    "    hidden_dimension = Waa.shape[0]\n",
    "\n",
    "    Wxa_update = 0\n",
    "    Waa_update = 0\n",
    "    Way_update = 0\n",
    "    \n",
    "    next_input_hidden_delta = np.zeros((1, hidden_dimension))\n",
    "\n",
    "    for t in range(binary_dim):\n",
    "        predicted = y_values[-t - 1]\n",
    "\n",
    "        y = np.array([[c[t]]])\n",
    "\n",
    "        error += np.abs(np.mean(y * np.log(predicted) + (1 - y) * np.log(1 - predicted)))\n",
    "        dJ_dz3 = -(y - predicted)\n",
    "\n",
    "        x = np.array([[a[t], b[t]]])\n",
    "        a2 = hidden_values[-t - 1]\n",
    "        prev_a2 = hidden_values[-t - 2]\n",
    "\n",
    "        dJ_da2 = next_input_hidden_delta @ Waa.T + dJ_dz3 @ Way.T\n",
    "        dJ_dz2 = dJ_da2 * (a2 * (1-a2))\n",
    "\n",
    "        Way_update += a2.T @ dJ_dz3\n",
    "        Waa_update += prev_a2.T @ dJ_dz2\n",
    "        Wxa_update += x.T @ dJ_dz2\n",
    "\n",
    "        next_input_hidden_delta = dJ_dz2\n",
    "\n",
    "    W_updates = (Wxa_update, Waa_update, Way_update)\n",
    "\n",
    "    return W_updates, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "After backpropagation we can update weights by substracting from them those gradients multiplied by hyperparamter called learning rate, which defines how fast our model is learning itself.\n",
    "\n",
    "<h3> Steps for training </h3>\n",
    "$W_{ax} := W_{ax} - learning\\_rate \\cdot \\frac{\\delta J}{\\delta W_{ax}}$\n",
    "\n",
    "$W_{aa} := W_{aa} - learning\\_rate \\cdot \\frac{\\delta J}{\\delta W_{aa}}$\n",
    "\n",
    "$W_{ay} := W_{ay} - learning\\_rate \\cdot \\frac{\\delta J}{\\delta W_{ay}}$\n",
    "\n",
    "\n",
    "0 - Initialize weights with random values\n",
    "\n",
    "For k times (where k is number of epochs)\n",
    "\n",
    "1. Process forward pass to get values at layers\n",
    "2. Process backward pass to get error at weights\n",
    "3. Update weights to be closer to Cost function minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(int2binary, binary_dimension, input_dimension=2, \n",
    "          hidden_dimension=16, output_dimension=1, epochs=10_000, learning_rate=1e-3):\n",
    "    '''\n",
    "    param int2binary: dictionary that mpas integer numbers into binary\n",
    "\n",
    "    param binary_dimension: max length of binary number\n",
    "\n",
    "    param input_dimension: number of nodes in input layer for single execution\n",
    "\n",
    "    param hidden_dimension: nubmer of nodes in hidden layer\n",
    "\n",
    "    param output_dimension: number of nodes in output layer for single execution\n",
    "\n",
    "    param epochs: number of epochs\n",
    "    \n",
    "    param learning rate: hyperparameter - rate of learning speed\n",
    "    \n",
    "    return: funcion return weights Wxa, Waa, Way and costs\n",
    "    '''\n",
    "\n",
    "    largest_number = pow(2, binary_dimension)\n",
    "\n",
    "\n",
    "    Wxa = 2 * np.random.random((input_dimension, hidden_dimension)) - 1\n",
    "    Waa = 2 * np.random.random((hidden_dimension, hidden_dimension)) - 1\n",
    "    Way = 2 * np.random.random((hidden_dimension, output_dimension)) - 1\n",
    "\n",
    "    costs_in_batch = []\n",
    "    costs = []\n",
    "\n",
    "    for epoch in tqdm_notebook(range(epochs)):\n",
    "        # first binary number\n",
    "        a_int = np.random.randint(largest_number / 2)  # int version\n",
    "        a = int2binary[a_int]\n",
    "\n",
    "        # second binary number\n",
    "        b_int = np.random.randint(largest_number / 2)  # int version\n",
    "        b = int2binary[b_int]\n",
    "\n",
    "        # true answer\n",
    "        c_int = a_int + b_int\n",
    "        c = int2binary[c_int]\n",
    "\n",
    "        W = (Wxa, Waa, Way)\n",
    "\n",
    "        y_values, hidden_values = feed_forward(a, b, W)\n",
    "\n",
    "        W_updates, cost = back_propagation(a, b, c, W, y_values, hidden_values)\n",
    "\n",
    "        costs_in_batch.append(cost)\n",
    "\n",
    "        if epoch % 500 == 1:\n",
    "            mean_in_batch = reduce(lambda x, y: x + y, costs_in_batch) / len(costs_in_batch)\n",
    "            costs.append(mean_in_batch)\n",
    "            costs_in_batch = []\n",
    "\n",
    "        Wxa_update, Waa_update, Way_update = W_updates\n",
    "\n",
    "        Wxa -= Wxa_update * learning_rate\n",
    "        Waa -= Waa_update * learning_rate\n",
    "        Way -= Way_update * learning_rate\n",
    "\n",
    "\n",
    "\n",
    "    W = Wxa, Waa, Way\n",
    "\n",
    "    return W, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Prediction is a single feed forward part for each input pair, to generate expected sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(int2binary, binary_dim, W):\n",
    "    '''\n",
    "    param int2binary: dictionary that maps integer values into binary\n",
    "    param binary_dim: maximum length of binary number\n",
    "    param W: tuple of weights Wxa, Waa, Way\n",
    "    \n",
    "    '''\n",
    "    Wxa, Waa, Way = W\n",
    "\n",
    "\n",
    "    largest_number = pow(2, binary_dim)\n",
    "\n",
    "    # first binary number\n",
    "    a_int = np.random.randint(largest_number / 2)  # int version\n",
    "    a = int2binary[a_int]\n",
    "\n",
    "    # second binary number\n",
    "    b_int = np.random.randint(largest_number / 2)  # int version\n",
    "    b = int2binary[b_int]\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "\n",
    "    hidden_dimension = Waa.shape[0]\n",
    "\n",
    "    prediction = np.zeros_like(c)\n",
    "\n",
    "    hidden_values = [np.zeros((1, hidden_dimension))]\n",
    "\n",
    "    for t in range(binary_dim):\n",
    "        x_t = np.array([[a[binary_dim - t - 1], b[binary_dim - t - 1]]])\n",
    "\n",
    "        z_t = x_t @ Wxa + hidden_values[-1] @ Waa\n",
    "        a_t = sigmoid(z_t)\n",
    "\n",
    "        h_t = a_t @ Way\n",
    "        y_t = sigmoid(h_t)\n",
    "\n",
    "        prediction[binary_dim - t - 1] = np.round(y_t)\n",
    "\n",
    "        hidden_values.append(a_t)\n",
    "\n",
    "    print(\"Predicted:\" + str(prediction))\n",
    "    print(\"True:\" + str(c))\n",
    "\n",
    "    out = 0\n",
    "    for index, x in enumerate(reversed(prediction)):\n",
    "        out += x * pow(2, index)\n",
    "    print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2febae52ea9543df9df9d0f4c51c2b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:[1 0 0 1 1 0 1 1]\n",
      "True:[1 0 0 1 1 0 1 1]\n",
      "39 + 116 = 155\n",
      "Predicted:[1 1 0 1 0 0 0 1]\n",
      "True:[1 1 0 1 0 0 0 1]\n",
      "83 + 126 = 209\n",
      "Predicted:[0 1 0 1 1 0 0 1]\n",
      "True:[0 1 0 1 1 0 0 1]\n",
      "68 + 21 = 89\n",
      "Predicted:[0 1 0 0 1 1 0 1]\n",
      "True:[0 1 0 0 1 1 0 1]\n",
      "28 + 49 = 77\n",
      "Predicted:[0 1 1 1 1 1 1 0]\n",
      "True:[0 1 1 1 1 1 1 0]\n",
      "24 + 102 = 126\n",
      "Predicted:[1 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 0 0]\n",
      "11 + 117 = 128\n",
      "Predicted:[1 0 1 1 0 1 0 0]\n",
      "True:[1 0 1 1 0 1 0 0]\n",
      "83 + 97 = 180\n",
      "Predicted:[1 0 0 1 0 0 1 1]\n",
      "True:[1 0 0 1 0 0 1 1]\n",
      "44 + 103 = 147\n",
      "Predicted:[0 1 0 1 1 1 0 1]\n",
      "True:[0 1 0 1 1 1 0 1]\n",
      "79 + 14 = 93\n",
      "Predicted:[0 0 1 1 1 0 1 1]\n",
      "True:[0 0 1 1 1 0 1 1]\n",
      "19 + 40 = 59\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "binary_dim = 8\n",
    "largest_number = pow(2, binary_dim)\n",
    "int2binary = create_int2bin_dict(binary_dim)\n",
    "\n",
    "W, costs = train(int2binary, binary_dim, epochs=20_000, learning_rate=1e-2)\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.xlabel('Training time')\n",
    "plt.ylabel('Cost value')\n",
    "plt.title('Cost')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "for i in range(10):\n",
    "    predict(int2binary, binary_dim, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "\n",
    "Followed sources were usefull during creating this notebook:\n",
    "\n",
    "https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/, Andrew Trask the whole idea about summing up binaries, gifs about feed forward and backpropagation.\n",
    "\n",
    "https://magenta.tensorflow.org/2016/06/10/recurrent-neural-network-generation-tutorial, scheme image of recurrent neural network\n",
    "\n",
    "https://www.coursera.org/specializations/deep-learning, Prof. Andrew Ng - examples of RNN usage \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
